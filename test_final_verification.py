#!/usr/bin/env python3
"""
Final verification script for the non-PDF file upload fixes.
This simulates the exact scenario that was failing before.
"""

import os
import sys
import json

# Add the project root to sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

def simulate_azure_search_document():
    """Simulate creating a document for Azure Search upload like the real pipeline"""
    
    print("üß™ SIMULATING AZURE SEARCH DOCUMENT CREATION")
    print("=" * 60)
    
    # Simulate a PowerPoint chunk that was causing the error
    pptx_chunk = {
        "id": "pptx_chunk_id_123",
        "content": "UltraDisk new features overview\n‚Ä¢ Enhanced performance\n‚Ä¢ Better scalability\n‚Ä¢ Cost optimization", 
        "page_number": 1,
        "source_file": "04. UltraDisk new features.pptx",
        # No imageCaptions - this was causing captionVector=None
    }
    
    # Simulate a Word document chunk  
    docx_chunk = {
        "id": "docx_chunk_id_456",
        "content": "◊û◊°◊ú◊ï◊ú◊ô◊ù ◊¶◊§◊ï◊ü ◊ê◊ô◊ò◊ô◊ú◊ô◊î\n◊™◊ô◊ê◊ï◊® ◊î◊û◊°◊ú◊ï◊ú◊ô◊ù ◊î◊û◊ï◊û◊ú◊¶◊ô◊ù ◊ú◊ò◊ô◊ï◊ú ◊ë◊¶◊§◊ï◊ü ◊ê◊ô◊ò◊ô◊ú◊ô◊î",
        "page_number": 1,
        "source_file": "◊û◊°◊ú◊ï◊ú◊ô◊ù ◊¶◊§◊ï◊ü ◊ê◊ô◊ò◊ô◊ú◊ô◊î.docx",
        "imageCaptions": [],  # Empty list - also caused issues
        "relatedImages": []
    }
    
    print("üìÑ Processing PowerPoint chunk...")
    doc1 = create_search_document(pptx_chunk, "04. UltraDisk new features.pptx")
    print(f"‚úÖ PowerPoint document created: ID={doc1['id']}")
    print(f"   - captionVector type: {type(doc1['captionVector'])}")
    print(f"   - captionVector length: {len(doc1['captionVector']) if isinstance(doc1['captionVector'], list) else 'N/A'}")
    print(f"   - captionVector is None: {doc1['captionVector'] is None}")
    
    print("\nüìÑ Processing Word document chunk...")
    doc2 = create_search_document(docx_chunk, "◊û◊°◊ú◊ï◊ú◊ô◊ù ◊¶◊§◊ï◊ü ◊ê◊ô◊ò◊ô◊ú◊ô◊î.docx")
    print(f"‚úÖ Word document created: ID={doc2['id']}")
    print(f"   - captionVector type: {type(doc2['captionVector'])}")
    print(f"   - captionVector length: {len(doc2['captionVector']) if isinstance(doc2['captionVector'], list) else 'N/A'}")
    print(f"   - captionVector is None: {doc2['captionVector'] is None}")
    
    # Verify both documents would pass Azure Search validation
    success = True
    for i, doc in enumerate([doc1, doc2], 1):
        if doc['captionVector'] is None:
            print(f"‚ùå Document {i}: captionVector is None - would cause Azure Search error!")
            success = False
        elif not isinstance(doc['captionVector'], list):
            print(f"‚ùå Document {i}: captionVector is not a list - would cause Azure Search error!")
            success = False
        elif len(doc['captionVector']) != 3072:
            print(f"‚ùå Document {i}: captionVector has wrong dimensions - would cause Azure Search error!")
            success = False
    
    if success:
        print("\nüéâ SUCCESS: Both documents have valid captionVectors!")
        print("‚úÖ Non-PDF files should now upload to Azure Search without errors.")
        print("‚úÖ The 'null value found for captionVector' error should be resolved.")
    else:
        print("\n‚ùå FAILURE: Documents still have captionVector issues!")
    
    return success

def create_search_document(chunk, filename):
    """Simulate the document creation logic from agentic-rag-demo.py"""
    
    # This simulates the FIXED logic from agentic-rag-demo.py
    import hashlib
    
    # Generate caption vector for multimodal content
    caption_vector = [0.0] * 3072  # Default to zero vector (THE FIX!)
    image_captions = chunk.get("imageCaptions", [])
    if image_captions and isinstance(image_captions, list) and len(image_captions) > 0:
        # Use safe join to handle mixed types (strings, dicts, etc.)
        captions_text = " ".join([str(cap) for cap in image_captions if cap])
        if captions_text:
            # In real scenario, this would call embed_text()
            print(f"   üìä Would generate embedding for captions: {captions_text}")
            caption_vector = [0.1] * 3072  # Simulate real embedding
    
    # Create the document structure that matches Azure Search schema
    doc = {
        "id": chunk.get("id") or hashlib.md5(f"{filename}_chunk".encode()).hexdigest(),
        "page_chunk": chunk.get("content", ""),
        "page_embedding_text_3_large": [0.0] * 3072,  # Simulated content embedding
        "content": chunk.get("content", ""),
        "contentVector": [0.0] * 3072,  # Simulated content vector
        "page_number": chunk.get("page_number", 1),
        "source_file": filename,
        "source": filename,
        "url": "",
        "extraction_method": "document_intelligence",
        "document_type": "Office Document",
        "has_figures": bool(chunk.get("relatedImages")),
        "processing_timestamp": "2025-06-21T15:35:00Z",
        # Multimodal fields
        "imageCaptions": " ".join([str(cap) for cap in image_captions]) if image_captions else "",
        "captionVector": caption_vector,  # This was None before - NOW it's a zero vector!
        "relatedImages": chunk.get("relatedImages", []),
        "isMultimodal": bool(image_captions or chunk.get("relatedImages")),
        "filename": filename,
    }
    
    return doc

if __name__ == "__main__":
    print("üîç FINAL VERIFICATION: Non-PDF File Upload Fix")
    success = simulate_azure_search_document()
    print("\n" + "=" * 60)
    print("SUMMARY:")
    print("- Issue: PowerPoint/Word files caused 'null value for captionVector' errors")
    print("- Root cause: captionVector was set to None for non-multimodal content")
    print("- Fix: Set captionVector to zero vector [0.0] * 3072 instead of None")
    print("- Result: Azure Search accepts zero vectors, preventing upload errors")
    print("=" * 60)
    sys.exit(0 if success else 1)
