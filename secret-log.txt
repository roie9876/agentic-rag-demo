commit 8dd540231a704f603e9996bc403d5b423268838e
Author: roie ben haim <roie9876@gmail.com>
Date:   Mon Jun 16 22:49:20 2025 +0300

    update agent

diff --git a/agentic-rag-demo.py b/agentic-rag-demo.py
index 21b6673..97e36dd 100644
--- a/agentic-rag-demo.py
+++ b/agentic-rag-demo.py
@@ -1088,7 +1088,7 @@ def run_streamlit_ui() -> None:
                                     "required": True,
                                     "schema": {
                                         "type": "string",
-                                        "default": "ax279AHHcMaWwdNGsjpgMxsufgFPuuwxxyRA3zeIzd6QAzFuXusxpA=="
+                                        "default": ""
                                     },
                                     "description": "Function host key"
                                 },
@@ -1122,10 +1122,10 @@ def run_streamlit_ui() -> None:
                 "You have one action called Test_askAgentFunction.\n"
                 "Call it **every time** the user asks a factual question.\n"
                 "Send the whole question unchanged as the {question} path parameter **and** include the two query parameters exactly as shown below:\n"
-                "  • code=ax279AHHcMaWwdNGsjpgMxsufgFPuuwxxyRA3zeIzd6QAzFuXusxpA==\n"
+                "  • code=\n"
                 "  • includesrc=true\n"
                 "Example URL you must generate (line breaks added for clarity):\n"
-                "POST https://agenticfun.azurewebsites.net/api/AgentFunction/{question}?code=ax279AHHcMaWwdNGsjpgMxsufgFPuuwxxyRA3zeIzd6QAzFuXusxpA==&includesrc=true\n"
+                "POST https://agenticfun.azurewebsites.net/api/AgentFunction/{question}?code=&includesrc=true\n"
                 "Return the Function's plain‑text response **verbatim and in full**, including any inline citations such as [my_document.pdf].\n"
                 "Do **NOT** add, remove, reorder, or paraphrase content, and do **NOT** drop those citation markers.\n"
                 "If the action fails, reply exactly with: I don't know\n"

commit d36ff4327efd179df510c883fbd75ff2eae272a8
Author: roie ben haim <roie9876@gmail.com>
Date:   Mon Jun 16 22:42:13 2025 +0300

    working ai foundry agent

diff --git a/agentic-rag-demo.py b/agentic-rag-demo.py
index bc7a6a9..21b6673 100644
--- a/agentic-rag-demo.py
+++ b/agentic-rag-demo.py
@@ -28,10 +28,10 @@ except ImportError:       # fallback for older Streamlit
     _st_in_runtime = lambda: False
 
 import re  # for citation parsing
-
+from azure.identity import DefaultAzureCredential
 from openai import AzureOpenAI
 from azure.core.credentials import AzureKeyCredential
-from azure.core.exceptions import ClientAuthenticationError
+from azure.core.exceptions import ClientAuthenticationError, HttpResponseError
 from azure.search.documents import SearchClient
 from azure.search.documents.indexes import SearchIndexClient  # NEW
 from azure.search.documents.indexes.models import (
@@ -57,6 +57,7 @@ from azure.search.documents.indexes.models import (
 
 # --- Logging setup ---
 import logging
+import inspect
 # Show only warnings and errors in the terminal
 logging.basicConfig(level=logging.WARNING, format="%(levelname)s: %(message)s")
 
@@ -73,7 +74,32 @@ from dotenv import load_dotenv
 from azure.search.documents import SearchIndexingBufferedSender  # NEW
 import fitz                            # PyMuPDF
 import hashlib, tempfile               # for PDF processing
-from azure.identity import DefaultAzureCredential, get_bearer_token_provider
+from azure.identity import AzureCliCredential, get_bearer_token_provider
+from azure.ai.projects import AIProjectClient
+# --- Azure AI Foundry SDK ----------------------------------------------------
+# Support multiple SDK generations where the tool classes moved packages/names
+try:
+    # GA / recent preview: everything under azure.ai.agents
+    from azure.ai.agents import FunctionTool, FunctionDefinition
+except ImportError:
+    # Older builds may expose FunctionTool and/or FunctionDefinition
+    # under azure.ai.agents.models
+    try:
+        from azure.ai.agents import FunctionTool  # type: ignore
+    except ImportError:
+        FunctionTool = None  # type: ignore
+    try:
+        from azure.ai.agents.models import FunctionTool as _FTModel, FunctionDefinition  # type: ignore
+        if FunctionTool is None:  # fallback when only the models version exists
+            FunctionTool = _FTModel  # type: ignore
+    except ImportError:
+        FunctionDefinition = None  # type: ignore
+
+# OpenAPI tool helper (available in azure‑ai‑agents ≥ 1.0.0b2)
+from azure.ai.agents.models import OpenApiTool, OpenApiAnonymousAuthDetails
+
+
+# ToolDefinition is only under .models
 def _search_credential() -> AzureKeyCredential | DefaultAzureCredential:
     """
     Return Azure credential based on env:
@@ -117,6 +143,53 @@ def _az_logged_user() -> tuple[str | None, str | None]:
         return None, None
 
 
+def check_azure_cli_login() -> tuple[bool, dict | None]:
+    """
+    Return (logged_in, account_json_or_None) by running `az account show`.
+    """
+    try:
+        out = subprocess.check_output(
+            ["az", "account", "show", "--output", "json"],
+            text=True,
+            timeout=5,
+        )
+        return True, json.loads(out)
+    except subprocess.CalledProcessError:
+        return False, None
+    except Exception:
+        return False, None
+
+
+def get_ai_foundry_projects(cred: AzureCliCredential) -> list[dict]:
+    """
+    Return a list of Foundry projects visible to the signed‑in CLI user via
+    `az ai project list`. Each item includes:
+        {name, location, endpoint, resource_group, hub_name}
+    """
+    try:
+        out = subprocess.check_output(
+            ["az", "ai", "project", "list", "--output", "json"],
+            text=True,
+            timeout=10,
+        )
+        data = json.loads(out)
+        projs = []
+        for p in data:
+            projs.append(
+                {
+                    "name": p["name"],
+                    "location": p["location"],
+                    "endpoint": p["properties"]["endpoint"],
+                    "resource_group": p["resourceGroup"],
+                    "hub_name": p["properties"].get("hubName", ""),
+                }
+            )
+        return projs
+    except Exception as err:
+        logging.warning("Failed to list AI Foundry projects: %s", err)
+        return []
+
+
 def _grant_search_role(service_name: str, subscription_id: str, resource_group: str, principal: str, role: str) -> tuple[bool, str]:
     """
     Grant the specified *role* to *principal* on the given service.
@@ -270,6 +343,7 @@ def pdf_to_documents(pdf_file, oai_client: AzureOpenAI, embed_deployment: str) -
             "page_embedding_text_3_large": vector,
             "page_number": page_num + 1,
             "source_file": pdf_file.name,
+            "source": pdf_file.name,             # extra alias recognised by agent
             "url": pdf_url,
         }
         docs.append(doc)
@@ -366,6 +440,8 @@ def create_agentic_rag_index(index_client: "SearchIndexClient", name: str) -> bo
                             facetable=True),
                 SimpleField(name="source_file", type="Edm.String",
                             filterable=True, facetable=True),
+                SimpleField(name="source", type="Edm.String",
+                            filterable=True, facetable=True),
                 SimpleField(name="url", type="Edm.String",
                             filterable=False),
             ],
@@ -529,13 +605,102 @@ def answer(question: str, ctx: str, client: AzureOpenAI, params: dict) -> tuple[
 # CLI entry‑point
 ##############################################################################
 
+##############################################################################
+# Pipeline‑as‑a‑Tool helper: wraps KnowledgeAgentRetrievalClient.retrieve
+# ---------------------------------------------------------------------------
+def agentic_retrieval(agent_name: str, index_name: str, messages: list[dict]) -> str:
+    """
+    מבצע שליפה סוכנתית (Agentic Retrieval) באמצעות KnowledgeAgentRetrievalClient עבור agent נתון ו-index נתון.
+    """
+    # הגנה על פורמט ההודעות: המרה לסchema תקינה
+    fixed_msgs = []
+    for m in messages:
+        if isinstance(m, dict) and "role" in m and "content" in m:
+            fixed_msgs.append(m)
+        elif isinstance(m, str):
+            fixed_msgs.append({"role": "user", "content": m})
+        else:
+            raise ValueError(f"Unknown message format: {m}")
+
+    ka_client = KnowledgeAgentRetrievalClient(
+        endpoint=env("AZURE_SEARCH_ENDPOINT"),
+        agent_name=agent_name,
+        credential=_search_credential(),
+    )
+    ka_msgs = [
+        KnowledgeAgentMessage(
+            role=m["role"],
+            content=[KnowledgeAgentMessageTextContent(text=m["content"])]
+        )
+        for m in fixed_msgs
+    ]
+    # ------------------ build retrieval request ---------------------------
+    # If caller supplies an index_name, try to force the query there; otherwise
+    # fall back to the agent’s default target index.
+    target_params: list[KnowledgeAgentIndexParams] | None = None
+    if index_name:
+        target_params = [
+            KnowledgeAgentIndexParams(
+                index_name=index_name,
+                reranker_threshold=2.5,
+            )
+        ]
+
+    req = KnowledgeAgentRetrievalRequest(
+        messages=ka_msgs,
+        # Only include target_index_params if we actually specified one
+        target_index_params=target_params,
+        request_limits=KnowledgeAgentRequestLimits(max_output_size=6000),
+    )
+
+    # ------------------ execute – retry without explicit index on mismatch -
+    try:
+        result = ka_client.knowledge_retrieval.retrieve(retrieval_request=req)
+    except HttpResponseError as err:
+        # If the agent is not configured for *index_name*, retry letting the
+        # agent use its default target index.
+        if (
+            "target index name must match" in str(err).lower()
+            and target_params is not None
+        ):
+            req.target_index_params = None  # remove the conflicting override
+            result = ka_client.knowledge_retrieval.retrieve(retrieval_request=req)
+        else:
+            raise  # re‑raise unrelated errors
+
+    # ----------------------------------------------------------------------
+    # Build a rich JSON array with metadata so downstream agents can show
+    # proper citations (url / source_file / page_number).
+    # Each chunk inside `result.response` is a KnowledgeAgentMessage object
+    # that holds one or more `content` items.
+    # We flatten everything into a list like:
+    #   [{"ref_id": 0, "content": "...", "url": "...", "source_file": "...", "page_number": 3}, …]
+    # ----------------------------------------------------------------------
+    chunks: list[dict] = []
+    for msg in result.response:
+        for c in getattr(msg, "content", []):
+            chunk = {
+                # ref_id might be absent – fall back to running index
+                "ref_id": getattr(c, "ref_id", None) or len(chunks),
+                "content": getattr(c, "text", ""),
+                "url": getattr(c, "url", None),
+                "source_file": getattr(c, "source_file", None),
+                "page_number": getattr(c, "page_number", None),
+                "score": getattr(c, "score", None),
+            }
+            # prune empty keys
+            chunks.append({k: v for k, v in chunk.items() if v is not None})
+
+    # Return the raw JSON string (no extra formatting)
+    return json.dumps(chunks, ensure_ascii=False)
+
 # -----------------------------------------------------------------------------
 # Streamlit UI wrapper (run with: streamlit run agentic-rag-demo.py)
 # -----------------------------------------------------------------------------
 def run_streamlit_ui() -> None:
     st.set_page_config(page_title="Agentic RAG Demo", page_icon="📚", layout="wide")
 
-    # --- ensure session keys exist ---
+    # ── persistent session keys ───────────────────────────────────────────
     for k, default in {
         "selected_index": None,
         "available_indexes": [],
@@ -543,32 +708,28 @@ def run_streamlit_ui() -> None:
         "indexed_documents": {},
         "history": [],
         "agent_messages": [],
+        "dbg_chunks": 0,
+        "orchestrator_targets": {},  # mapping: orchestrator agent → retrieval agent
     }.items():
         st.session_state.setdefault(k, default)
 
     st.title("📚 Agentic Retrieval‑Augmented Chat")
-    # --- global LTR helper ---------------------------------------------------
     st.markdown(
         """
         <style>
-        /* Force the entire app to LTR */
         html, body, .stApp { direction: ltr; text-align: left; }
-        /* Keep per-message helper for explicit blocks */
         .ltr { direction: ltr; text-align: left; }
         </style>
         """,
         unsafe_allow_html=True,
     )
 
-    # Sidebar – choose model
+    # ── Sidebar – model & RAG knobs ───────────────────────────────────────
     with st.sidebar:
-        # Model is fixed to GPT-4.1; dropdown removed
-        st.header("⚙️ Model: GPT-4.1")
+        st.header("⚙️ Model: GPT‑4.1")
         model_choice = "41"
-        # ---------------------------------------------------
-        # Single OpenAI client for this UI run (needed by PDF-ingest too)
         oai_client, chat_params = init_openai(model_choice)
-        # ---------------------------------------------------
+
         st.caption("Change `.env` to add more deployments")
         auth_mode = "Azure AD" if not os.getenv("AZURE_SEARCH_KEY") else "API Key"
         st.caption(f"🔑 Search auth: {auth_mode}")
@@ -579,129 +740,51 @@ def run_streamlit_ui() -> None:
                 "Turn on **Role‑based access control (Azure RBAC)** under "
                 "*Search service → Networking → Authentication*."
             )
-        # Classic RAG option removed – the app always uses the Knowledge-Agent pipeline
-        st.subheader("🛠️ RAG Parameters")
-        # How many documents to keep in context
-        st.session_state.ctx_size = st.slider("Context chars per chunk",
-                                              min_value=300, max_value=2000,
-                                              value=600, step=50)
-        # How many hits to retrieve per sub‑query
-        st.session_state.top_k = st.slider("TOP‑K per query",
-                                           min_value=1, max_value=200,
-                                           value=5, step=1)
-        # Reranker threshold for Knowledge‑Agent
-        st.session_state.rerank_thr = st.slider("Reranker threshold",
-                                                min_value=0.0, max_value=4.0,
-                                                value=2.0, step=0.1)
-        # Maximum JSON‑chunk size returned by the Knowledge‑Agent (maxOutputSize)
-        st.session_state.max_output_size = st.slider("Knowledge‑agent maxOutputSize",
-                                                     min_value=1000, max_value=16000,
-                                                     value=5000, step=500)
-
-        # --- Apply new maxOutputSize to existing Knowledge‑Agent -----------------
-        if st.session_state.selected_index:
-            if st.button("💾 Apply maxOutputSize", help="Update the selected index's Knowledge‑Agent"):
-                try:
-                    # Fetch current agent, update the request_limits and push the change
-                    _, _icl = init_search_client()  # index client for service root
-                    agent_name_sel = f"{st.session_state.selected_index}-agent"
-                    agent_obj = _icl.get_agent(agent_name_sel)
-                    agent_obj.request_limits = KnowledgeAgentRequestLimits(
-                        max_output_size=int(st.session_state.max_output_size)
-                    )
-                    _icl.create_or_update_agent(agent_obj)
-                    st.success(f"Knowledge‑Agent **{agent_name_sel}** updated "
-                               f"to maxOutputSize = {st.session_state.max_output_size} B")
-                except Exception as ex:
-                    st.error(f"Failed to update agent limits: {ex}")
-        else:
-            st.caption("👉 Create / select an index to enable agent‑limit update")
 
-        # Max completion tokens for chat responses
-        st.session_state.max_tokens = st.slider("Max completion tokens",
-                                                min_value=256, max_value=32768,
-                                                value=32768, step=256)
+        st.subheader("🛠️ RAG Parameters")
+        st.session_state.ctx_size = st.slider("Context chars per chunk", 300, 2000, 600, 50)
+        st.session_state.top_k = st.slider("TOP‑K per query", 1, 200, 5, 1)
+        st.session_state.rerank_thr = st.slider("Reranker threshold", 0.0, 4.0, 2.0, 0.1)
+        st.session_state.max_output_size = st.slider("Knowledge‑agent maxOutputSize", 1000, 16000, 5000, 500)
+        st.session_state.max_tokens = st.slider("Max completion tokens", 256, 32768, 32768, 256)
 
-        # Live chunk counter placeholder
         chunks_placeholder = st.empty()
         chunks_placeholder.caption(f"Chunks sent to LLM: {st.session_state.get('dbg_chunks', 0)}")
 
-        # Reload .env button
         if st.button("🔄 Reload .env & restart"):
             _reload_env_and_restart()
 
-        st.divider()
-        st.header("🔒 RBAC Helper")
-
-        user_upn, sub_id = _az_logged_user()
-        if user_upn:
-            st.markdown(f"**Signed‑in user:** {user_upn}")
-            st.markdown(f"**Subscription:** {sub_id}")
-            with st.expander("Grant 'Search Service Contributor' on this Search service", expanded=False):
-                svc_name = env("AZURE_SEARCH_ENDPOINT").split("://")[1].split(".")[0]
-                rg_input = st.text_input("Resource Group name", placeholder="my-search-rg", key="search_rg")
-                principal = st.text_input("Assignee (leave blank = me)", value=user_upn, key="search_principal")
-                if st.button("⚙️ Grant role"):
-                    if not rg_input:
-                        st.error("Please enter the Resource Group")
-                    else:
-                        roles = [
-                            "Search Service Contributor",
-                            "Search Index Data Contributor",
-                        ]
-                        ok_all = True
-                        messages = []
-                        for r in roles:
-                            ok, msg = _grant_search_role(svc_name, sub_id, rg_input, principal or user_upn, r)
-                            ok_all &= ok
-                            messages.append(f"{r}: {msg}")
-                        if ok_all:
-                            st.success("✓ ".join(messages))
-                        else:
-                            st.error(" | ".join(messages))
+    # ── Tabbed layout ─────────────────────────────────────────────────────
+    tab_create, tab_manage, tab_test = st.tabs(
+        [
+            "1️⃣ Create Index",
+            "2️⃣ Manage Index",
+            "3️⃣ Test Retrieval"
+        ]
+    )
+    tab_ai = st.tabs(["🤖 AI Foundry Agent"])[0]
 
-            st.divider()
-            st.header("🛡️ OpenAI Role Helper")
+    # Service‑root client used across tabs
+    _, root_index_client = init_search_client()
 
-            # Try to derive account name from endpoint (use 41 as canonical)
-            default_ep = os.getenv("AZURE_OPENAI_ENDPOINT_41") or os.getenv("AZURE_OPENAI_ENDPOINT", "")
-            default_account = ""
-            if default_ep:
-                try:
-                    default_account = default_ep.split("https://")[1].split(".")[0]
-                except Exception:
-                    pass
-
-            with st.expander("Grant 'Cognitive Services OpenAI User' on this OpenAI account", expanded=False):
-                oai_account = st.text_input("OpenAI account name", value=default_account, placeholder="my-openai")
-                oai_rg = st.text_input("Resource Group name", placeholder="my-openai-rg", key="openai_rg")
-                oai_principal = st.text_input("Assignee (leave blank = me)", value=user_upn or "", key="openai_principal")
-                role_choice = st.selectbox("Role",
-                                           options=["Cognitive Services OpenAI User", "Cognitive Services Contributor"],
-                                           index=0)
-                if st.button("⚙️ Grant OpenAI role"):
-                    if not (oai_account and oai_rg):
-                        st.error("Enter both Account name and Resource Group.")
-                    else:
-                        ok, msg = _grant_openai_role(
-                            oai_account, sub_id, oai_rg, oai_principal or user_upn, role_choice
-                        )
-                        (st.success if ok else st.error)(msg)
-        else:
-            st.info("User not logged via az cli ‑ run `az login` in the terminal.")
+    # ─────────────────── Tab 1 – Create Index ────────────────────────────
+    with tab_create:
+        st.header("🆕 Create a New Vector Index")
+        new_index_name = st.text_input("New index name", placeholder="e.g. agentic‑vectors")
+        if st.button("➕ Create new index") and new_index_name:
+            if create_agentic_rag_index(root_index_client, new_index_name):
+                st.success(f"Created index '{new_index_name}'")
+                st.session_state.selected_index = new_index_name
+                if new_index_name not in st.session_state.available_indexes:
+                    st.session_state.available_indexes.append(new_index_name)
 
-    # ---------------- Index management -----------------
-    st.header("📚 Index Management")
+    # ─────────────────── Tab 2 – Manage Index ────────────────────────────
+    with tab_manage:
+        st.header("📂 Manage Existing Index")
 
-    # Refresh list of indexes each time sidebar is drawn
-    _, sidebar_index_client = init_search_client()   # no index yet
-    # After refreshing, show warning if no indexes found
-    if not st.session_state.available_indexes:
-        st.warning("⚠️ No indexes found or insufficient permissions. "
-                   "Check Search Index Data Reader role.")
+        # refresh list each render
+        st.session_state.available_indexes = [idx.name for idx in root_index_client.list_indexes()]
 
-    col1, col2 = st.columns(2)
-    with col1:
         existing = st.selectbox(
             "Existing indexes",
             options=[""] + st.session_state.available_indexes,
@@ -714,140 +797,121 @@ def run_streamlit_ui() -> None:
             st.session_state.selected_index = existing
             st.success(f"Selected index: {existing}")
 
-    with col2:
-        new_index_name = st.text_input("New index name", placeholder="e.g. agentic-vectors")
-        if st.button("➕ Create new index") and new_index_name:
-            if create_agentic_rag_index(sidebar_index_client, new_index_name):
-                st.success(f"Created index '{new_index_name}'")
-                st.session_state.selected_index = new_index_name
-                st.session_state.available_indexes.append(new_index_name)
-                st.rerun()   # refresh UI so sidebar shows maxOutputSize button
-        # --- NEW: delete selected index & its agent ---------------------------------
-    if st.session_state.selected_index:
-        st.warning(f"Selected index: **{st.session_state.selected_index}**")
-        if st.button("🗑️ Delete selected index", type="secondary"):
-            try:
-                _, _icl = init_search_client()  # service-root index client
-                idx_name   = st.session_state.selected_index
-                agent_name = f"{idx_name}-agent"
-
-                # 1️⃣  Delete the referencing agent (ignore error if absent)
+        # delete selected
+        if st.session_state.selected_index:
+            st.warning(f"Selected index: **{st.session_state.selected_index}**")
+            if st.button("🗑️ Delete selected index"):
                 try:
-                    _icl.delete_agent(agent_name)
-                except Exception:
-                    pass  # agent might not exist or insufficient rights
+                    idx_name = st.session_state.selected_index
+                    agent_name = f"{idx_name}-agent"
+                    try:
+                        root_index_client.delete_agent(agent_name)
+                    except Exception:
+                        pass
+                    root_index_client.delete_index(idx_name)
+                    st.session_state.available_indexes.remove(idx_name)
+                    st.session_state.selected_index = None
+                    st.success(f"Deleted index **{idx_name}** and its agent.")
+                except Exception as ex:
+                    st.error(f"Failed to delete index: {ex}")
 
-                # 2️⃣  Delete the index now that no agent references it
-                if idx_name in [i.name for i in _icl.list_indexes()]:
-                    _icl.delete_index(idx_name)
+        st.divider()
+        st.subheader("📄 Upload PDFs into Selected Index")
+        if not st.session_state.selected_index:
+            st.info("Select an index first.")
+        else:
+            uploaded = st.file_uploader("Choose PDF files", type=["pdf"], accept_multiple_files=True)
+            if uploaded and st.button("🚀 Ingest"):
+                with st.spinner("Embedding and uploading…"):
+                    ###############################################
+                    # Build buffered sender with error‑tracking
+                    ###############################################
+                    failed_ids: list[str] = []
+
+                    def _on_error(action) -> None:
+                        """
+                        Callback for SearchIndexingBufferedSender — called once per failed
+                        indexing action. *action* is the document that failed.
+
+                        We record its ID (if present) so the UI can report how many pages
+                        were skipped.
+                        """
+                        try:
+                            # `action` is usually the original document (dict) we provided
+                            failed_ids.append(action.get("id", "?"))
+                        except Exception as exc:
+                            logging.error("⚠️  on_error callback failed to record ID: %s", exc)
+                            failed_ids.append("?")
 
-                # Update UI state
-                st.session_state.available_indexes = [
-                    n for n in st.session_state.available_indexes if n != idx_name
-                ]
-                st.session_state.selected_index = None
-                st.success(f"Deleted index **{idx_name}** and its agent.")
-                st.rerun()   # refresh UI to remove maxOutputSize button
-            except Exception as ex:
-                st.error(f"Failed to delete index: {ex}")
-
-    # ---------------- PDF uploader + SharePoint URL ingest -----------------
-    st.subheader("📄 Upload PDFs to index")
-    if not st.session_state.selected_index:
-        st.info("Select or create an index first.")
-    else:
-        uploaded = st.file_uploader("Choose local PDF files",
-                                    type=["pdf"], accept_multiple_files=True)
-
-        # --- SharePoint URL input ----------------------------------------
-        sharepoint_urls = st.text_area(
-            "Or paste SharePoint/HTTP links (one per line)",
-            placeholder="https://contoso.sharepoint.com/....pdf",
-            height=100,
-        ).strip().splitlines()
-
-        if uploaded or any(sharepoint_urls):
-            if st.button("🚀 Ingest PDFs"):
-                with st.spinner("Embedding and uploading..."):
-                    _, iclient = init_search_client(st.session_state.selected_index)
                     sender = SearchIndexingBufferedSender(
                         endpoint=env("AZURE_SEARCH_ENDPOINT"),
                         index_name=st.session_state.selected_index,
                         credential=_search_credential(),
+                        # Flush every 100 docs or every 5 s – whichever comes first
+                        batch_size=100,
+                        auto_flush_interval=5,
+                        on_error=_on_error,
                     )
-                    embed_deploy = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")
-                    total = 0
 
-                    # Local files
+                    embed_deploy = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-large")
+                    total_pages = 0
                     for pf in uploaded:
                         docs = pdf_to_documents(pf, oai_client, embed_deploy)
                         sender.upload_documents(documents=docs)
-                        total += len(docs)
-
-                    # SharePoint / HTTP files
-                    for url in filter(None, sharepoint_urls):
-                        try:
-                            pdf_bytes, fname = _download_pdf(url)
-                            # Wrap BytesIO to mimic Streamlit's UploadedFile
-                            pdf_bytes.name = fname
-                            fake_file = types.SimpleNamespace(name=fname, getbuffer=pdf_bytes.getbuffer)
-                            docs = pdf_to_documents(fake_file, oai_client, embed_deploy)
-                            # Overwrite 'url' with the original link for every page
-                            for d in docs:
-                                d["url"] = url
-                            sender.upload_documents(documents=docs)
-                            total += len(docs)
-                        except Exception as dl_err:
-                            st.error(f"Failed to ingest {url}: {dl_err}")
+                        total_pages += len(docs)
 
+                    # Ensure everything is sent
                     sender.close()
-                    st.success(f"Uploaded {total} pages into '{st.session_state.selected_index}'.")
 
-    #--- build / refresh clients ------------------------------------------------
-    search_client, _ = init_search_client(st.session_state.selected_index)
-
-    # Chat history
-    if "history" not in st.session_state:
-        st.session_state.history = []
+                    ###############################################
+                    # Optional: wait until the documents are searchable
+                    ###############################################
+                    try:
+                        search_client, _ = init_search_client(st.session_state.selected_index)
+                        # Basic probe – wait until at least one document shows up
+                        import time
+                        for _ in range(30):                  # up to ~30 s
+                            if search_client.get_document_count() > 0:
+                                break
+                            time.sleep(1)
+                    except Exception as probe_err:
+                        logging.warning("Search probe failed: %s", probe_err)
+
+                    ###############################################
+                    # Report outcome
+                    ###############################################
+                    success_pages = total_pages - len(failed_ids)
+                    if failed_ids:
+                        st.error(f"❌ {len(failed_ids)} pages failed to index – see logs for details.")
+                    if success_pages:
+                        st.success(f"✅ Indexed {success_pages} pages into **{st.session_state.selected_index}**.")
+
+    # ─────────────────── Tab 3 – Test Retrieval ──────────────────────────
+    with tab_test:
+        st.header("🧪 Test Retrieval Without Foundry")
+        if not st.session_state.selected_index:
+            st.info("Select or create an index in the previous tabs.")
+        else:
+            search_client, _ = init_search_client(st.session_state.selected_index)
 
-    # Display history
-    for turn in st.session_state.history:
-        with st.chat_message(turn["role"]):
-            st.markdown(f'<div class="ltr">{turn["content"]}</div>', unsafe_allow_html=True)
+            # History
+            for turn in st.session_state.history:
+                with st.chat_message(turn["role"]):
+                    st.markdown(f'<div class="ltr">{turn["content"]}</div>', unsafe_allow_html=True)
 
-    user_query = st.chat_input("Ask your question…")
-    if user_query:
-        st.session_state.history.append({"role": "user", "content": user_query})
-        with st.chat_message("user"):
-            st.markdown(f'<div class="ltr">{user_query}</div>', unsafe_allow_html=True)
+            user_query = st.chat_input("Ask your question…")
+            if user_query:
+                st.session_state.history.append({"role": "user", "content": user_query})
+                with st.chat_message("user"):
+                    st.markdown(f'<div class="ltr">{user_query}</div>', unsafe_allow_html=True)
 
-        # Always run the agentic path (requires a selected index)
-        if st.session_state.selected_index:
-            with st.spinner("Knowledge‑agent retrieval…"):
                 agent_name = f"{st.session_state.selected_index}-agent"
                 agent_client = init_agent_client(agent_name)
-
-                # ------------------------------------------------------------------
-                # Conversational instructions (persist for the whole session)
-                instr = (
-                    "Answer the question based only on the indexed sources. "
-                    "Cite ref_id in square brackets. If unknown, answer \"I don't know\"."
-                )
-
-                # 1️⃣  Build / update running message history
                 if not st.session_state.agent_messages:
-                    # first turn – seed with system‑style assistant instruction
-                    st.session_state.agent_messages = [
-                        {"role": "assistant", "content": instr}
-                    ]
-
-                # append current user question
-                st.session_state.agent_messages.append(
-                    {"role": "user", "content": user_query}
-                )
+                    st.session_state.agent_messages = [{"role": "assistant", "content": "Answer with sources."}]
+                st.session_state.agent_messages.append({"role": "user", "content": user_query})
 
-                # convert to SDK objects
-                message_objs = [
+                ka_msgs = [
                     KnowledgeAgentMessage(
                         role=m["role"],
                         content=[KnowledgeAgentMessageTextContent(text=m["content"])]
@@ -856,131 +920,251 @@ def run_streamlit_ui() -> None:
                 ]
 
                 ka_req = KnowledgeAgentRetrievalRequest(
-                    messages=message_objs,
-                    target_index_params=[
-                        KnowledgeAgentIndexParams(
-                            index_name=st.session_state.selected_index,
-                            reranker_threshold=float(st.session_state.rerank_thr)
-                        )
-                    ],
+                    messages=ka_msgs,
+                    target_index_params=[KnowledgeAgentIndexParams(
+                        index_name=st.session_state.selected_index,
+                        reranker_threshold=float(st.session_state.rerank_thr)
+                    )],
                     request_limits=KnowledgeAgentRequestLimits(
                         max_output_size=int(st.session_state.max_output_size)
                     ),
                 )
-                try:
+                with st.spinner("Retrieving…"):
                     result = agent_client.knowledge_retrieval.retrieve(retrieval_request=ka_req)
-                except ClientAuthenticationError as err:
-                    st.error(
-                        "❌ Unauthorized (401).\n\n"
-                        "• Make sure **Role‑based access control** is enabled on the service.\n"
-                        "• Assign yourself **Search Service Contributor** and "
-                        "**Search Index Data Contributor** roles.\n"
-                        "• Or temporarily set an Admin key in `AZURE_SEARCH_KEY`."
-                    )
-                    st.exception(err)
-                    return
-                # ------------------------------------------------------------------
-                # 1) Low‑level response (chunks) -----------------------------------
-                raw_text = result.response[0].content[0].text   # JSON or plain text
 
-                # Persist chunks as the assistant's turn for future questions
-                st.session_state.agent_messages.append(
-                    {"role": "assistant", "content": raw_text}
-                )
+                raw_text = result.response[0].content[0].text
+                st.session_state.agent_messages.append({"role": "assistant", "content": raw_text})
 
-                # Try to parse the JSON list that the knowledge‑agent usually emits
+                # ── Handle raw_text coming from the Function/Agent ───────────
                 try:
-                    parsed = json.loads(raw_text)
-                    if isinstance(parsed, list):
-                        ctx_for_llm = "\n\n".join(
-                            f"[doc{item.get('ref_id')}] {item.get('content','')}"
-                            for item in parsed
-                        )
-                    else:
-                        # agent returned plain text – use as is
-                        ctx_for_llm = parsed
+                    parsed_json = json.loads(raw_text)
                 except Exception:
-                    ctx_for_llm = raw_text
+                    parsed_json = None
+
+                answer_text = None          # will hold the final answer we print
+                sources_data = []           # unified list → [{source_file,url,...}, ...]
 
                 # ------------------------------------------------------------------
-                # 2) Pass the chunks to OpenAI for **final answer generation**
-                num_chunks = len(result.response)
-                st.session_state.dbg_chunks = num_chunks
-                chunks_placeholder.caption(f"Chunks sent to LLM: {num_chunks}")
-                answer_text, usage_tok = answer(user_query, ctx_for_llm, oai_client, chat_params)
+                # 1) Case A – Function returned {"answer": "...", "sources": [...] }
+                # ------------------------------------------------------------------
+                if isinstance(parsed_json, dict) and "answer" in parsed_json:
+                    answer_text  = parsed_json.get("answer", "").strip()
+                    sources_data = parsed_json.get("sources", [])
+                    # treat the answer as already‑final → no extra LLM step
+                    chunk_count = 1
+                    ctx_for_llm = None
 
                 # ------------------------------------------------------------------
-                # 3) Build docs list for “Sources” pane from result.references
-                docs = [
-                    {"id": i + 1, "url": ref.doc_key, "page_chunk": ""}
-                    for i, ref in enumerate(result.references)
-                ]
-                # --- low‑level reference diagnostics ---
-                for ref in result.references:
-                    logging.warning(
-                        "KA ref: doc_key=%s  reranker_score=%s",
-                        getattr(ref, "doc_key", None),
-                        getattr(ref, "reranker_score", None),
+                # 2) Case B – Function returned a *list* of chunks (old style)
+                # ------------------------------------------------------------------
+                elif isinstance(parsed_json, list):
+                    ctx_for_llm = "\n\n".join(
+                        f"[doc{item.get('ref_id','?')}] {item.get('content','')}"
+                        for item in parsed_json
                     )
-                logging.warning("KA references returned: %s", len(result.references))
-                # --- diagnostics ---
-                unique_sources = {ref.doc_key for ref in result.references}
-                st.caption(f" Unique sources in answer: {len(unique_sources)}")
-
-                chunk_bytes = len(raw_text.encode("utf-8"))
-                logging.warning("KA output size: %s bytes", chunk_bytes)
+                    chunk_count = len(parsed_json)
 
-                # 4) Optional: expose Activity & Results in UI
-                with st.expander("⚙️  Retrieval activity"):
-                    st.json([a.as_dict() for a in result.activity], expanded=False)
+                # ------------------------------------------------------------------
+                # 3) Case C – plain text (fallback)
+                # ------------------------------------------------------------------
+                else:
+                    ctx_for_llm  = raw_text
+                    chunk_count  = 1
 
-                with st.expander("📑  Raw references"):
-                    st.json([r.as_dict() for r in result.references], expanded=False)
+                # For sidebar diagnostic
+                st.session_state.dbg_chunks = chunk_count
 
-                # Enrich docs with original PDF file name
-                for d in docs:
-                    try:
-                        full_doc = search_client.get_document(key=d["url"])
-                        d["source_file"] = full_doc.get("source_file", "")
-                        d["url"]         = full_doc.get("url", d.get("url", ""))
-                    except Exception:
-                        d["source_file"] = ""
+                # ------------------------------------------------------------------
+                # When we still need to summarise with the LLM (cases B & C)
+                # ------------------------------------------------------------------
+                if answer_text is None:
+                    answer_text, usage_tok = answer(user_query, ctx_for_llm, oai_client, chat_params)
+                else:
+                    # We already have the final text (case A)
+                    usage_tok = 0
+                # ------------------------------------------------------------------
 
-        else:
-            st.warning("Select an index to enable retrieval.")
-
-        with st.chat_message("assistant"):
-            st.markdown(f'<div class="ltr">{answer_text}</div>', unsafe_allow_html=True)
-            if 'usage_tok' in locals():
-                st.caption(f"_Tokens used: {usage_tok}_")
-
-        # ---- Sources block -------------------------------------------------
-        cited_ids = {int(m.group(1)) for m in re.finditer(r"\[doc(\d+)]", answer_text)}
-        if cited_ids:
-            with st.expander("📚 Sources", expanded=False):
-                for cid in sorted(cited_ids):
-                    doc = next((d for d in docs if d["id"] == cid), None)
-                    if not doc:
-                        continue
-                    page = doc.get("page_number", "")
-                    page_str = f"(page {page})" if page else ""
-                    url  = doc.get("url") or doc.get("source", "")
-                    name = doc.get("source_file", "")
-                    label = name if name else f"doc{cid}"
-                    if url:
-                        st.markdown(f"**{label}** {page_str} — [{url}]({url})")
-                    else:
-                        st.markdown(f"**{label}** {page_str}")
-                    st.write(doc.get("page_chunk", doc.get("content", ""))[:500] + "…")
-                    st.divider()
+                with st.chat_message("assistant"):
+                    st.markdown(answer_text, unsafe_allow_html=True)
+                    st.caption(f"_Tokens used: {usage_tok}_")
+
+                # ── Show concise list of sources / citations ─────────────────────
+                if not sources_data and isinstance(parsed_json, list):
+                    # legacy list → build minimal list from chunks
+                    for item in parsed_json:
+                        src_name = (
+                            item.get("source_file")
+                            or item.get("source")
+                            or item.get("url")
+                            or f"doc{item.get('ref_id', '?')}"
+                        )
+                        sources_data.append(
+                            {"source_file": src_name, "url": item.get("url", "")}
+                        )
 
-        # --------------------------------------------------------------------
+                if sources_data:
+                    st.markdown("#### 🗂️ Sources")
+                    for src in sources_data:
+                        name = src.get("source_file") or "unknown source"
+                        url  = src.get("url", "")
+                        if url:
+                            st.markdown(f"- [{name}]({url})")
+                        else:
+                            st.markdown(f"- {name}")
+
+                # --- Optional: show raw chunks -----------------------------------
+                if isinstance(parsed_json, list) and parsed_json:
+                    with st.expander("📚 מקורות (chunks)", expanded=False):
+                        for item in parsed_json:
+                            ref = item.get("ref_id", item.get("id", '?'))
+                            txt = item.get("content", "")
+                            st.markdown(f"**📄 מקור {ref}:**")
+                            st.write(txt)
+                            st.markdown("---")
+
+
+    # ─────────────────── Single Tab – AI Foundry Agent ──────────────────
+    with tab_ai:
+        st.header("🤖 Create AI Foundry Agent")
+
+        # Detect Foundry projects the CLI user can access
+        cli_cred = AzureCliCredential()
+        logged_in, _ = check_azure_cli_login()
+        if not logged_in:
+            st.error("🔑 Run `az login` before using this feature.")
+            st.stop()
+
+        projects = get_ai_foundry_projects(cli_cred)
+        if not projects and os.getenv("PROJECT_ENDPOINT"):
+            ep = os.getenv("PROJECT_ENDPOINT").strip()
+            projects = [{
+                "name": ep.split('/')[-1][:30] or "env-project",
+                "location": "env",
+                "endpoint": ep,
+                "resource_group": "env",
+                "hub_name": "env",
+            }]
+
+        if not projects:
+            st.warning("No AI Foundry projects detected.")
+            st.stop()
+
+        proj_labels = [f"{p['name']} – {p['location']}" for p in projects]
+        sel = st.selectbox("Choose Foundry project", proj_labels, index=0)
+        project_endpoint = projects[proj_labels.index(sel)]['endpoint']
+        st.caption(f"🔗 Endpoint: {project_endpoint}")
+
+        agent_name = st.text_input("Agent name", placeholder="function‑assistant")
+        if st.button("🚀 Create Agent") and agent_name:
+            # --- Build anonymous OpenAPI tool -----------------------------------
+            TOOL_NAME = "Test_askAgentFunction"
+            tool_schema = {
+                "openapi": "3.0.1",
+                "info": {
+                    "title": "AgentFunction",
+                    "version": "1.0.0"
+                },
+                # Base URL for the Function App (no query‑string here!)
+                "servers": [
+                    {
+                        "url": "https://agenticfun.azurewebsites.net/api"
+                    }
+                ],
+                "paths": {
+                    "/AgentFunction/{question}": {
+                        "post": {
+                            "operationId": "askAgentFunction",
+                            "summary": "Ask the Azure Function",
+                            "parameters": [
+                                {
+                                    "name": "question",
+                                    "in": "path",
+                                    "required": True,
+                                    "schema": {"type": "string"}
+                                },
+                                {
+                                    "name": "code",
+                                    "in": "query",
+                                    "required": True,
+                                    "schema": {
+                                        "type": "string",
+                                        "default": "ax279AHHcMaWwdNGsjpgMxsufgFPuuwxxyRA3zeIzd6QAzFuXusxpA=="
+                                    },
+                                    "description": "Function host key"
+                                },
+                                {
+                                    "name": "includesrc",
+                                    "in": "query",
+                                    "required": False,
+                                    "schema": {
+                                        "type": "boolean",
+                                        "default": True
+                                    },
+                                    "description": "Include sources in the Function response"
+                                }
+                            ],
+                            "responses": {
+                                "200": {
+                                    "description": "Plain‑text answer",
+                                    "content": {
+                                        "text/plain": {
+                                            "schema": {"type": "string"}
+                                        }
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+
+            SYSTEM_MSG = (
+                "You have one action called Test_askAgentFunction.\n"
+                "Call it **every time** the user asks a factual question.\n"
+                "Send the whole question unchanged as the {question} path parameter **and** include the two query parameters exactly as shown below:\n"
+                "  • code=ax279AHHcMaWwdNGsjpgMxsufgFPuuwxxyRA3zeIzd6QAzFuXusxpA==\n"
+                "  • includesrc=true\n"
+                "Example URL you must generate (line breaks added for clarity):\n"
+                "POST https://agenticfun.azurewebsites.net/api/AgentFunction/{question}?code=ax279AHHcMaWwdNGsjpgMxsufgFPuuwxxyRA3zeIzd6QAzFuXusxpA==&includesrc=true\n"
+                "Return the Function's plain‑text response **verbatim and in full**, including any inline citations such as [my_document.pdf].\n"
+                "Do **NOT** add, remove, reorder, or paraphrase content, and do **NOT** drop those citation markers.\n"
+                "If the action fails, reply exactly with: I don't know\n"
+                "Do **NOT** answer from your own internal knowledge and do **NOT** answer questions unrelated to the Function.\n"
+                "\n"
+                "### How to respond\n"
+                "1. Parse the JSON the Function returns.\n"
+                "2. Reply with the **exact value of \"answer\"** – do NOT change it.\n"
+                "3. After that, print a short “Sources:” list. For each object in \"sources\" show its **source_file** (fallback to url if empty; if both missing, show the placeholder doc#).\n"
+                "   Example:\n"
+                "   Sources:\n"
+                "   • המב 50.02.pdf\n"
+                "   • מס 40.021.pdf\n"
+            )
 
-        st.session_state.history.append({"role": "assistant", "content": answer_text})
+            # --- Create OpenAPI tool (anonymous auth) ---------------------------
+            auth = OpenApiAnonymousAuthDetails()  # public endpoint – no key required
+            openapi_tool = OpenApiTool(
+                name=TOOL_NAME,
+                spec=tool_schema,
+                description="Invoke the Azure Function via HTTP POST",
+                auth=auth,
+            )
 
-        # Simplified debug section (classic RAG variables no longer exist)
-        with st.expander("🔍 Debug info"):
-            st.caption("Agentic pipeline active – classic RAG code removed.")
+            # Create the agent via the Azure AI Projects SDK
+            try:
+                proj_client = AIProjectClient(project_endpoint, DefaultAzureCredential())
+                with proj_client:
+                    agent = proj_client.agents.create_agent(
+                        name=agent_name,
+                        model="gpt-4.1",                   # make sure this deployment exists
+                        instructions=SYSTEM_MSG,
+                        description="Assistant created from Streamlit UI",
+                        tools=openapi_tool.definitions,   # <-- note: *definitions*
+                    )
+                st.success(f"✅ Agent **{agent.name}** created (ID: {agent.id})")
+            except Exception as err:
+                st.error("Failed to create agent via SDK:")
+                st.exception(err)
 
 
 def main() -> None:
